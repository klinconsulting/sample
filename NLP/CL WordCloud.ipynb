{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0f22fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee02e8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nycdoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nycdoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nycdoe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK stopwords data\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfc472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('CL WordCloud.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca1838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Summary By CI']= df.groupby('Change Idea')['Summary'].transform(lambda x: ' '.join(x))\n",
    "df = df[['Change Idea','Summary By CI']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c97577a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Change Idea</th>\n",
       "      <th>Summary By CI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Change idea: 4- Collaborative Learning</td>\n",
       "      <td>The change idea implemented in this text is th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Change idea: 5- Student Identities</td>\n",
       "      <td>The change idea being implemented is incorpora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Change idea: 2- Language Demands</td>\n",
       "      <td>The change idea implemented by the F.I.R.E. te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Change idea: 1- Academic Vocabulary</td>\n",
       "      <td>The change idea implemented in this text is te...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Change Idea  \\\n",
       "0  Change idea: 4- Collaborative Learning   \n",
       "1      Change idea: 5- Student Identities   \n",
       "2        Change idea: 2- Language Demands   \n",
       "3     Change idea: 1- Academic Vocabulary   \n",
       "\n",
       "                                       Summary By CI  \n",
       "0  The change idea implemented in this text is th...  \n",
       "1  The change idea being implemented is incorpora...  \n",
       "2  The change idea implemented by the F.I.R.E. te...  \n",
       "3  The change idea implemented in this text is te...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648184e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "069c8e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordcloud_1- Academic Vocabulary.png\n",
      "wordcloud_2- Language Demands.png\n",
      "wordcloud_4- Collaborative Learning.png\n",
      "wordcloud_5- Student Identities.png\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Define a function to generate WordCloud for a given text and save to file\n",
    "def generate_and_save_wordcloud(text, filename,change_idea,title_fontsize=20, title_padding=6,dpi=800,n_gram=3, word_count=10):\n",
    "    \n",
    "    # Apply preprocessing to the text\n",
    "    text = preprocess_text(text)\n",
    "    \n",
    "    \n",
    "    # Generate verb_grams from processed text (e.g., between 2 to 4 words)\n",
    "    min_words = 3\n",
    "    max_words = 4\n",
    "    # Generate n-grams from processed text\n",
    "    #phrases = generate_verb_ngrams(text, n_gram)  # Change 3 to 4 for 4-word phrases, and so on\n",
    "    phrases = generate_verb_grams_v2(text, min_words, max_words)\n",
    "    \n",
    "    \n",
    "    # Generate n-grams from text\n",
    "    #phrases = generate_ngrams(text, n_gram)  # Change 2 to 3 for 3-word phrases\n",
    "\n",
    "    # Calculate frequencies\n",
    "    #phrase_freq = {phrase: phrases.count(phrase) for phrase in set(phrases)}\n",
    "\n",
    "    # Calculate frequencies\n",
    "    phrase_freq = Counter(phrases)\n",
    "\n",
    "    # Select the top 30 most frequent verb_phrases\n",
    "    final_phrases = dict(phrase_freq.most_common(30))\n",
    "    \n",
    "    \n",
    "    # wordcloud = WordCloud(width=400, height=400, background_color='white').generate(text)\n",
    "    wordcloud = WordCloud(width=400, height=400, background_color='white').generate_from_frequencies(final_phrases)\n",
    "    \n",
    "     # Create a subplot with adjustable spacing between title and wordcloud\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    plt.subplots_adjust(top=0.8 + title_padding/100)  # Adjust the top spacing\n",
    "    \n",
    "    ax.imshow(wordcloud, interpolation='bilinear')\n",
    "    ax.set_title(change_idea, fontsize=title_fontsize, pad=title_padding)  # Increase title fontsize and add padding\n",
    "    ax.axis('off')\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.savefig(filename,dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "# Function to generate n-grams from text\n",
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "# Function to remove stopwords and punctuation\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    \n",
    "    # Tokenize and remove stopwords and punctuation\n",
    "    tokens = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
    "    tokens = [word.translate(translator) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to check if a phrase contains a verb\n",
    "def contains_verb(phrase):\n",
    "    tags = pos_tag(word_tokenize(phrase))\n",
    "    return any(tag.startswith('VB') for _, tag in tags)\n",
    "\n",
    "# Function to generate n-grams from text containing a verb\n",
    "def generate_verb_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1) if contains_verb(' '.join(words[i:i+n]))]\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "# Function to generate verb_grams from text with a specified word range\n",
    "def generate_verb_grams_v2(text, min_words, max_words):\n",
    "    words = text.split()\n",
    "    verb_grams = [\n",
    "        tuple(words[i:i+n]) \n",
    "        for n in range(min_words, max_words + 1) \n",
    "        for i in range(len(words)-n+1) \n",
    "        if contains_verb(' '.join(words[i:i+n]))\n",
    "    ]\n",
    "    return [' '.join(gram) for gram in verb_grams]\n",
    "\n",
    "# Step 3: Iterate through groups and generate WordCloud for each 'grouped_summary'\n",
    "for change_idea, group_df in df.groupby('Change Idea'):\n",
    "    grouped_summary_text = ' '.join(group_df['Summary By CI'])\n",
    "\n",
    "    change_idea = change_idea.replace(\"Change idea: \", \"\")\n",
    "    filename = f'wordcloud_{change_idea}.png'  # Adjust the filename as needed\n",
    "    print(filename)\n",
    "    \n",
    "    generate_and_save_wordcloud(grouped_summary_text, filename, change_idea)\n",
    "    \n",
    "    #wordcloud = WordCloud(width=800, height=400, background_color='white').generate(grouped_summary_text)\n",
    "    #plt.figure(figsize=(10, 5))\n",
    "    #plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    #plt.axis('off')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b389b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
